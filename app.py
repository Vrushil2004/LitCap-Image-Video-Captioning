import os
import tempfile
from typing import Optional

import streamlit as st
from PIL import Image
import torch
import cv2
from transformers import BlipProcessor, BlipForConditionalGeneration

# ------------------------
# Load BLIP model (cached)
# ------------------------
@st.cache_resource(show_spinner=False)
def load_blip(model_name: str = "Salesforce/blip-image-captioning-base"):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    processor = BlipProcessor.from_pretrained(model_name)
    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)
    return processor, model, device

# ------------------------
# Caption generation
# ------------------------
def generate_caption(image: Image.Image, max_new_tokens: int = 20, num_beams: int = 3,
                     min_length: int = 5, repetition_penalty: float = 1.2) -> str:
    processor, model, device = load_blip()
    image = image.convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            min_length=min_length,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=2,    
            early_stopping=True        
        )
    caption = processor.decode(output_ids[0], skip_special_tokens=True).strip()
    return caption


# ------------------------
# Streamlit UI
# ------------------------
st.set_page_config(page_title="LiteCap ‚Äî Deep Captioning (BLIP)", page_icon="üñºÔ∏è")
st.title("LiteCap ‚Äî Deep Captioning (BLIP)")
st.write("Upload an **image or video** and get captions generated by a pretrained BLIP model.")

with st.expander("Advanced settings", expanded=False):
    max_new_tokens = st.slider("Max new tokens", 10, 50, 50, 5)
    num_beams = st.slider("Beam search (num_beams)", 1, 8, 3, 1)
    min_length = st.slider("Minimum caption length", 5, 30, 10, 1)
    repetition_penalty = st.slider("Repetition penalty", 1.0, 2.0, 1.05, 0.05)

uploaded = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "webp", "mp4", "avi", "mov", "mkv"])

# ------------------------
# Image Captioning
# ------------------------
if uploaded is not None and uploaded.type.startswith("image"):
    image = Image.open(uploaded)
    st.image(image, caption="Uploaded image", use_container_width=True)
    with st.spinner("Generating deep caption..."):
        try:
            caption = generate_caption(image, max_new_tokens, num_beams, min_length, repetition_penalty)
            st.success(caption)
        except Exception as e:
            st.error(f"Caption generation failed: {e}")

# ------------------------
# Video Captioning with Subtitles
# ------------------------
elif uploaded is not None and uploaded.type.startswith("video"):
    tfile = tempfile.NamedTemporaryFile(delete=False)
    tfile.write(uploaded.read())
    video_path = tfile.name

    st.video(video_path)  # show original uploaded video

    st.info("Processing video and generating captions (every 5 seconds)...")

    cap = cv2.VideoCapture(video_path)
    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))
    frame_interval = frame_rate * 5  # one frame every 5 seconds
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # output video with subtitles
    output_path = video_path + "_subtitled.mp4"
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))

    captions = []
    count = 0
    current_caption = ""
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # generate new caption at intervals
        if count % frame_interval == 0:
            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            try:
                current_caption = generate_caption(img, max_new_tokens, num_beams, min_length, repetition_penalty)
                captions.append(f" {count/frame_rate:.1f}s ‚Üí {current_caption}")
            except Exception:
                current_caption = "[Error]"

        # overlay caption on frame (bottom-center)
        if current_caption:
            text_size, _ = cv2.getTextSize(current_caption, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)
            text_x = (width - text_size[0]) // 2
            text_y = height - 50
            cv2.putText(
                frame,
                current_caption,
                (text_x, text_y),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (255, 255, 255),
                2,
                cv2.LINE_AA
            )
        out.write(frame)
        count += 1

    cap.release()
    out.release()

    st.success("Video processing complete with subtitles!")
    st.video(output_path)

    st.write("### Captions Timeline:")
    for c in captions:
        st.success(c)

else:
    st.info("Upload an **image (JPG, PNG, WEBP)** or **video (MP4, AVI, MOV, MKV)** to get captions.")

st.markdown("---")
st.caption("Tip: On first run, the BLIP model is downloaded from Hugging Face. Ensure internet access the first time. The app will use GPU if available, otherwise CPU.")
